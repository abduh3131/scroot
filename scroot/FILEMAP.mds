# scroot Folder Reference

This document walks through every artifact under `scroot/`, outlines how the pieces fit together, and explains the exact library versions the stack expects. Use it as a high-level map when onboarding new contributors or integrating the scooter pilot into a new platform.

## Top-Level Layout

| Path | Purpose |
| --- | --- |
| `README.md` | Operator-facing quick start guide for installing dependencies and launching the autonomy stack. |
| `REPORT.md` | Development report summarizing design decisions, testing status, and future work items. |
| `FILEMAP.md` | (This file) Complete description of modules, runtime flow, and dependency rationale. |
| `setup_scroot.py` | One-shot bootstrap script that creates a virtual environment, installs dependencies, and caches pretrained models. |
| `autonomy/` | Python package implementing sensing, perception, AI assistants, planning, control, and shared utilities. |
| `autonomy_launcher.py` | Single-entry launch script that wires CLI arguments into the `AutonomyPilot` orchestrator and performs dependency checks before boot. |
| `configs/` | Sample JSON specs for multi-sensor deployments (Jetson CSI + Rosmaster, etc.). |
| `sample_video.mp4` | Placeholder clip bundled for quick smoke tests—replace with real road footage or a live camera feed for meaningful runs. |

## Dependency Versions and Rationale

The stack is pinned to Python 3.10+ and depends on the following packages declared in `autonomy/requirements.txt`:

| Package | Minimum Version | Reason |
| --- | --- | --- |
| `ultralytics` | `>=8.1.0` | Provides YOLOv8 detector weights and runtime utilities with ONNX/TensorRT export compatibility for Jetson-class hardware. |
| `opencv-python` | `>=4.8.0` | Supplies camera capture, image resizing, and visualization primitives across desktop and Jetson builds. |
| `numpy` | `>=1.24.0` | Underpins numeric operations for occupancy map aggregation, filtering, and control smoothing. |
| `torch` | `>=2.1.0` | Required to execute YOLOv8 as well as the multimodal advisor (BLIP + FLAN) efficiently on CPU or CUDA. |
| `torchvision` | `>=0.16.0` | Bundles pre/post-processing transforms used by YOLO and image captioning models. |
| `transformers` | `>=4.37.0` | Loads Hugging Face language models (FLAN-T5) and multimodal pipelines for the situational advisor. |
| `accelerate` | `>=0.25.0` | Streamlines device placement and mixed-precision execution for the advisor on Jetson GPUs. |
| `sentencepiece` | `>=0.1.99` | Tokenizer dependency for FLAN-T5 language understanding. |
| `safetensors` | `>=0.3.1` | Speeds up secure model weight loading for BLIP/FLAN checkpoints. |

Jetson deployments can swap in vendor-optimized wheels as long as the versions meet or exceed these baselines.

## Python Package Structure (`autonomy/`)

The `autonomy` package is deliberately split into feature-oriented subpackages. Each module below can be tested or replaced independently.

### `autonomy/__init__.py`
Exports convenience imports so downstream code can instantiate the pilot, guardian, and mode manager components without digging into subpackages.

### `autonomy/pilot.py`
Central orchestrator that binds sensors, perception, navigation, advisor, and control. The `AutonomyPilot` class exposes a `run()` generator yielding raw `ActuatorCommand` tuples (`steer`, `throttle`, `brake`). It handles:
- Camera startup and graceful shutdown.
- Polling the natural-language `CommandInterface`.
- Passing frames through the `ObjectDetector` and `Navigator` to compute motion plans.
- Running the safety `Guardian` and high-level `ModeManager` to coordinate cruise, summon, idle, and emergency-stop transitions.
- Invoking the `SituationalAdvisor` to enrich decisions with traffic-law directives and emergency stops.
- Feeding `NavigationDecision` objects into the `Controller` for smoothing and actuator scaling.
- Persisting telemetry JSON, optional visualization overlays, annotated MP4 output, and CSV actuator logs mapped into MCU-friendly units.

### `autonomy/sensors/camera.py`
Defines `CameraSensor`, an OpenCV-backed capture wrapper that now auto-discovers USB devices, scans `/dev/video*`, and synthesizes Jetson CSI GStreamer pipelines with optional flip-method and probe controls.

### `autonomy/sensors/base.py`
Defines the abstract `Sensor`/`StreamingSensor` interfaces and the global sensor registry so new adapters (depth cameras, IMUs, ultrasonic arrays) can be discovered without touching the pilot.

### `autonomy/sensors/manager.py`
Creates a `SensorManager` that instantiates specs declared in configuration, starts/stops them as a group, and exposes unified `SensorSample` streams for the pilot, guardian, and localization modules.

### `autonomy/sensors/lidar.py`
Implements a configurable LiDAR adapter capable of replaying logged scans (JSON/CSV) or generating synthetic data for tests; registered under the `lidar` adapter key.

### `autonomy/sensors/rosmaster.py`
Serial bridge for the Yahboom Rosmaster X3 controller. Reads newline-delimited packets over `pyserial`, parses JSON or key/value telemetry, and exposes them to the pilot as structured sensor samples with automatic reconnection.

### `autonomy/perception/object_detection.py`
Wraps Ultralytics YOLO to produce `PerceptionSummary` objects containing detections, obstacle density metrics, and frame metadata. Configuration toggles allow adjusting model name, confidence, and IoU thresholds.

### `autonomy/ai/command_interface.py`
Parses human directives such as “drive 2 m forward” or “turn around” into structured `HighLevelCommand` dataclasses. Supports live updates via a watched text file, default commands, and exports of the latest parsed state.

### `autonomy/ai/advisor.py`
Implements the multimodal `SituationalAdvisor` that fuses YOLO detections with BLIP image captions and FLAN-T5 reasoning. It emits textual guidance (e.g., “use the bike lane”) and can set `enforced_stop` flags when detecting pedestrians, stop signs, or conflicting instructions.

### `autonomy/planning/navigator.py`
Holds the `Navigator`, which converts perception results and parsed commands into `NavigationDecision` outputs. Responsibilities include hazard scoring, speed targets, steering bias computation, and goal tracking for distance-based maneuvers.

### `autonomy/planning/localization.py`
Maintains a coarse localization confidence score derived from multi-sensor samples (e.g., LiDAR) so summon mode can respond to perception quality.

### `autonomy/planning/summon.py`
Summon planner that modulates navigation decisions based on localization quality while operating in `VehicleMode.SUMMON`.

### `autonomy/control/controller.py`
Translates navigation decisions into normalized actuator commands. Implements PID-style smoothing, braking prioritization during hazards or enforced stops, and ensures outputs stay within [-1, 1] bounds expected by downstream actuators.

### `autonomy/utils/data_structures.py`
Defines the shared dataclasses (`ActuatorCommand`, `PerceptionObject`, `NavigationDecision`, etc.) that flow between modules, ensuring type consistency and easy telemetry serialization.

### `autonomy/utils/filters.py`
Contains helper functions for exponential smoothing of speed/steering signals, reducing oscillations in tight spaces.

### `autonomy/core/mode_manager.py`
Defines `ModeManager` and `ModeManagerConfig`, enforcing cruise/summon/idle/emergency transitions and applying guardian overrides before actuator commands are generated.

### `autonomy/safety/guardian.py`
Implements the safety guardian that inspects perception and navigation outputs, produces slowdown/stop decisions, and emits accessibility alerts consumed by the pilot and downstream UIs.

## Documentation Assets

- `README.md` – End-user instructions for setup, runtime arguments, and operating practices.
- `REPORT.md` – Engineering log describing implementation rationale, testing steps, and follow-on roadmap suggestions.
- `FILEMAP.md` – Detailed architectural reference (this document).

## Launcher (`autonomy_launcher.py`)
The single entry point for operators. Responsibilities:
- Validates Python package dependencies before importing the heavy modules, providing actionable error messages if something is missing.
- Parses CLI arguments for camera setup, YOLO configuration, advisor toggles, and logging destinations.
- Instantiates `AutonomyPilot` and iterates through its command generator, printing normalized actuator triples together with MCU-mapped values and advisor guidance for integration with low-level motor controllers.

## Data Flow Summary

1. `autonomy_launcher.py` constructs `AutonomyPilot` using CLI-specified configuration.
2. The sensor registry instantiates the requested adapter (default `CameraSensor`) and streams frames; `ObjectDetector` converts them into detection lists.
3. `CommandInterface` delivers the latest high-level goal (if any).
4. `Navigator` evaluates detections plus goals to produce steering/speed intents.
5. `Guardian` inspects hazard scores and priority objects, emitting slowdowns or enforced stops for accessibility and safety.
6. `ModeManager` applies cruise/summon/idle policies before forwarding targets to the controller.
7. `SituationalAdvisor` (optional) analyzes frames and detections to append directives or trigger stops.
8. `Controller` generates final actuator outputs, which the launcher prints or forwards.
9. `AutonomyPilot` continuously exports JSON/video/CSV telemetry and respects stop signals for safe shutdowns.

With this map, new engineers can trace any actuator value back to its upstream source and adjust modules without breaking the surrounding system.
