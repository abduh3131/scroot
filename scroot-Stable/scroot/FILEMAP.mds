# scroot Folder Reference

This document walks through every artifact under `scroot/`, outlines how the pieces fit together, and explains the exact library versions the stack expects. Use it as a high-level map when onboarding new contributors or integrating the scooter pilot into a new platform.

## Top-Level Layout

| Path | Purpose |
| --- | --- |
| `README.md` | Operator-facing quick start guide for installing dependencies and launching the autonomy stack. |
| `REPORT.md` | Development report summarizing design decisions, testing status, and future work items. |
| `FILEMAP.md` | (This file) Complete description of modules, runtime flow, and dependency rationale. |
| `setup_scroot.py` | One-shot bootstrap script that creates a virtual environment, installs dependencies, and caches pretrained models. |
| `autonomy/` | Python package implementing sensing, perception, AI assistants, planning, control, and shared utilities. |
| `bridge/` | ROS helpers that mirror fused SensorHub topics into the runtime input files consumed by the AI. |
| `autonomy_launcher.py` | Single-entry launch script that wires CLI arguments into the `AutonomyPilot` orchestrator and performs dependency checks before boot. |
| `main.py` | Convenience wrapper so `python3 scroot/main.py` forwards to `autonomy_launcher.py` after dependency checks. |

## Dependency Versions and Rationale

The stack is pinned to Python 3.10+ and depends on the following packages declared in `autonomy/requirements.txt`:

| Package | Minimum Version | Reason |
| --- | --- | --- |
| `ultralytics` | `>=8.1.0` | Provides YOLOv8 detector weights and runtime utilities with ONNX/TensorRT export compatibility for Jetson-class hardware. |
| `opencv-python` | `>=4.8.0` | Supplies camera capture, image resizing, and visualization primitives across desktop and Jetson builds. |
| `numpy` | `>=1.24.0` | Underpins numeric operations for occupancy map aggregation, filtering, and control smoothing. |
| `torch` | `>=2.1.0` | Required to execute YOLOv8 efficiently on CPU or CUDA. |
| `torchvision` | `>=0.16.0` | Bundles pre/post-processing transforms used by YOLO and lane visualization utilities. |
| `psutil` | `>=5.9.0` | System monitoring used by the GUI to summarize hardware and runtime stats. |
| `Pillow` | `>=10.0.0` | Converts OpenCV frames into Tk-compatible images for the GUI. |

Jetson deployments can swap in vendor-optimized wheels as long as the versions meet or exceed these baselines.

## Python Package Structure (`autonomy/`)

The `autonomy` package is deliberately split into feature-oriented subpackages. Each module below can be tested or replaced independently.

### `autonomy/__init__.py`
Exports convenience imports so downstream code can instantiate `AutonomyPilot` and configuration dataclasses without deep module paths.

### `autonomy/pilot.py`
Central orchestrator that binds sensors, perception, lane detection, navigation, arbiter, and control. The `AutonomyPilot` class exposes a `run()` generator yielding raw `ActuatorCommand` tuples (`steer`, `throttle`, `brake`). It handles:
- Polling the runtime camera and LiDAR inputs written by the ROS bridge and shutting them down cleanly.
- Polling the natural-language `CommandInterface`.
- Passing frames through the `ObjectDetector` and `LaneDetector`, then into the `Navigator` to compute motion plans.
- Feeding `NavigationDecision` objects into the `Controller` and deterministic safety arbiter for smoothing and gating.
- Persisting telemetry JSON and optional visualization overlays.

### `autonomy/sensors/camera.py`
Defines `CameraSensor`, which now polls `~/scroot/runtime_inputs/camera.jpg` (while still supporting classic `VideoCapture` sources) and exposes a frame iterator with resolution/FPS controls plus throttled warnings when the runtime file has not been populated yet.

### `autonomy/sensors/lidar.py`
Provides `LidarSensor`, a helper that watches `~/scroot/runtime_inputs/lidar.npy`, loads the fused ROS ranges into NumPy arrays, and exposes them to the pilot so downstream planners/telemetry can read `AutonomyPilot.latest_lidar` without talking to ROS directly.

### `autonomy/perception/object_detection.py`
Wraps Ultralytics YOLO to produce `PerceptionSummary` objects containing detections, obstacle density metrics, and frame metadata. Configuration toggles allow adjusting model name, confidence, and IoU thresholds.

### `autonomy/perception/lane_detection.py`
Implements the `LaneDetector`, which applies HLS thresholds, perspective warps, sliding windows, and polynomial fits to reconstruct lane lines. Outputs lane width, curvature, confidence, and lateral offset for downstream modules.

### `autonomy/ai/command_interface.py`
Parses human directives such as “drive 2 m forward” or “turn around” into structured `HighLevelCommand` dataclasses. Supports live updates via a watched text file, default commands, and exports of the latest parsed state.

### `autonomy/planning/navigator.py`
Holds the `Navigator`, which converts perception results and parsed commands into `NavigationDecision` outputs. Responsibilities include hazard scoring, speed targets, steering bias computation, and goal tracking for distance-based maneuvers.

### `autonomy/control/controller.py`
Translates navigation decisions into normalized actuator commands. Implements PID-style smoothing, braking prioritization during hazards or enforced stops, and ensures outputs stay within [-1, 1] bounds expected by downstream actuators.

### `autonomy/utils/data_structures.py`
Defines the shared dataclasses (`ActuatorCommand`, `PerceptionObject`, `NavigationDecision`, etc.) that flow between modules, ensuring type consistency and easy telemetry serialization.

### `autonomy/utils/filters.py`
Contains helper functions for exponential smoothing of speed/steering signals, reducing oscillations in tight spaces.

## Documentation Assets

- `README.md` – End-user instructions for setup, runtime arguments, and operating practices.
- `REPORT.md` – Engineering log describing implementation rationale, testing steps, and follow-on roadmap suggestions.
- `FILEMAP.md` – Detailed architectural reference (this document).

## Launcher (`autonomy_launcher.py`)
The single entry point for operators. Responsibilities:
- Validates Python package dependencies before importing the heavy modules, providing actionable error messages if something is missing.
- Parses CLI arguments for camera setup, YOLO configuration, arbiter/lane toggles, and logging destinations.
- Instantiates `AutonomyPilot` and iterates through its command generator, printing each actuator triple alongside arbiter guidance for integration with low-level motor controllers.

## ROS Sensor Bridge (`bridge/ros_sensor_bridge.py`)
- Initializes the `ai_input_bridge` ROS node, subscribes to the fused `/sensor_hub/data` topic, and converts the camera frame via `CvBridge`.
- Saves the camera snapshot as `~/scroot/runtime_inputs/camera.jpg` and the LiDAR ranges as `~/scroot/runtime_inputs/lidar.npy`, auto-creating the directory when necessary.
- Throttles log spam to one message per second so the ROS console remains readable while the AI continuously consumes those runtime files.
- Acts as the glue between ROS and the autonomy stack: once the bridge is running, `python3 scroot/main.py` reads the same files without speaking ROS.

## Data Flow Summary

1. `bridge/ros_sensor_bridge.py` subscribes to `/sensor_hub/data` and continually updates `~/scroot/runtime_inputs/camera.jpg` + `lidar.npy`.
2. `autonomy_launcher.py` constructs `AutonomyPilot` using CLI-specified configuration, and the `CameraSensor`/`LidarSensor` pair read the runtime files produced by the bridge.
3. `ObjectDetector` and `LaneDetector` convert the fused camera frame into obstacle lists plus lane geometry.
4. `CommandInterface` delivers the latest high-level goal (if any).
5. `Navigator` evaluates detections, lane estimates, and goals to produce steering/speed intents.
6. The deterministic arbiter evaluates hazards, lane confidence, and mindset caps to ALLOW/AMEND/BLOCK proposed commands before they reach the `Controller`.
7. `Controller` generates final actuator outputs, which the launcher prints or forwards.
8. `AutonomyPilot` continuously exports JSON state for external monitoring and respects stop signals for safe shutdowns.

With this map, new engineers can trace any actuator value back to its upstream source and adjust modules without breaking the surrounding system.
